一.安装::
	使用tar包解压安装即可；


二.使用::
	默认的启动:
	./elasticsearch

	指定集群名称和节点名称的启动:
	./elasticsearch --cluster.name my_cluster_name --node.name my_node_name

	查看集群状态:
	curl 'localhost:9200/_cat/health?v'

	查看节点:
	curl 'localhost:9200/_cat/nodes?v'

	查看索引:
	curl 'localhost:9200/_cat/indices?v'

	创建索引:
	curl -XPUT 'localhost:9200/customer?pretty'

	索引一个文档:
	curl -XPUT 'localhost:9200/customer/external/1?pretty' -d '
	> {
	>   "name" : "arthur liu"
	> }'

	查询文档:
	curl -XGET 'localhost:9200/customer/external/1?pretty'
	查询文档只要source:
	curl -XGET 'localhost:9200/customer/external/1/_source'
	查询文档source中的一部分：
	curl -XGET 'localhost:9200/customer/external/1?_source=name,age&pretty'

	删除索引:
	curl -XDELETE 'localhost:9200/customer?pretty'

	索引一个文档,不主动提供ID:
	curl -XPOST 'localhost:9200/customer/external?pretty' -d '
	> {
	>     "name" : "nana.xu"
	> }'

	更新一个文档:
	curl -XPOST 'localhost:9200/customer/external/1/_update?pretty' -d '
	> {
	>     "doc" : {"name" : "wowowo.liu"}
	> }'

	更新一个文档,同时增加一个字段:
	curl -XPOST 'localhost:9200/customer/external/1/_update?pretty' -d '
	> {
	>     "doc" : {"name" : "ddmama.liushizhen", "age" : 20}
	> }'

	更新一个文档,通过脚本的方式:
	curl -XPOST 'localhost:9200/customer/external/1/_update?pretty' -d '
	>{
	>    "script" : "ctx._source.age += 5"
	> }'

	删除一个文档:
	curl -XDELETE 'localhost:9200/customer/external/2?pretty'

	删除文档,根据查询条件:
	curl -XDELETE 'localhost:9200/customer/external/_query?pretty' -d '
	> {
	>     "query" : {"match" : {"name" : "John"}}
	> }'

	批量索引一些文档:
	curl -XPOST 'localhost:9200/customer/external/_bulk?pretty' -d '
	> {"index" : {"_id" : "1"}}
	> {"name" : "arthur"}
	> {"index" : {"_id" : "4"}}
	> {"name" : "cherry"}
	> '
	批量索引一些文档:
	curl -XPOST 'localhost:9200/customer/external/_bulk?pretty' -d '
	> {"index" : {"_id" : "7"}}
	> {"name" : "xxxxx", "age" : 30, "sex" : "femail"}
	> {"index" : {"_id" : "8"}}
	> {"name" : "yyyyy", "sex" : "mail"}
	> '
	批量更新和删除一些文档:
	curl -XPOST 'localhost:9200/customer/external/_bulk?pretty' -d '
	> {"update" : {"_id" : "1"}}
	> {"doc" : {"name" : "arthur to be cherry"}}
	> {"delete" : {"_id" : "6"}}
	> '

	索引一大批数据到es,通过文件的方式:
	curl -XPOST 'localhost:9200/bank/account/_bulk?pretty' --data-binary @accounts.json

	查询一个索引中的所有文档(URI中传递参数的方式):
	curl 'localhost:9200/bank/_search?q=*&pretty'

	查询一个索引中的所有文档(请求体中传递参数的方式):
	curl -XPOST 'localhost:9200/bank/_search?pretty' -d '
	> {
	>   "query" : {"match_all" : {}}
	> }'


	*****一旦取回了搜索结果，Elasticsearch就完成了使命，它不会维护任何服务器端的资源或者在结果中打开游标*****

	查询一个索引中的所有文档,并限制返回条目数量(如果size没指定,默认为10):
	curl -XPOST 'localhost:9200/bank/_search?pretty' -d '
	> {
	>     "query" : {"match_all" : {}},
	>     "size" : 1
	> }'
	查询一个索引中的一些文档(如果from没指定,默认为0):
	curl -XPOST 'localhost:9200/bank/_search?pretty' -d '
	> {
	>     "query" : {"match_all" : {}},
	>     "from" : 10,
	>     "size" : 2
	> }'
	查询一个索引中的一些文档,排序后返回一部分:
	curl -XPOST 'localhost:9200/bank/_search?pretty' -d '
	> {
	>     "query" : {"match_all" : {}},
	>     "sort" : {"balance" : {"order" : "desc"}},
	>     "from" : 0,
	>     "size" : 11
	> }'


	查询一个索引中的一些文档,只查询某些字段:
	curl -XPOST 'localhost:9200/bank/_search?pretty' -d '
	> {
	>     "query" : {"match_all" : {}},
	>     "_source" : ["account_number", "balance"]
	> }'

	条件查询:
	curl -XPOST 'localhost:9200/bank/_search?pretty' -d '
	> {
	>     "query" : {"match" : {"account_number" : 20}}
	> }'
	条件查询部分字段:
	curl -XPOST 'localhost:9200/bank/_search?pretty' -d '
	> {
	>   "query" : {"match" : {"account_number" : 20}},
	>   "_source" : ["account_number", "balance"]
	> }'
	条件查询之"OR查询":
	curl -XPOST 'localhost:9200/bank/_search?pretty' -d '
	> {
	>   "query" : {"match" : {"address" : "mill lane"}}
	> }'
	条件查询之"全匹配查询":
	curl -XPOST 'localhost:9200/bank/_search?pretty' -d '
	> {
	>     "query" : {"match_phrase" : {"address" : "mill lane"}}
	> }'

	布尔查询之must查询:
	curl -XPOST 'localhost:9200/bank/_search?pretty' -d '
	> {
	>     "query" : {
	>         "bool" : {
	>             "must" : [
	>                 {"match" : {"address" : "mill"}},
	>                 {"match" : {"address" : "lane"}}
	>             ]
	>         }
	>     }
	> }'
	布尔查询之should查询:
	curl -XPOST 'localhost:9200/bank/_search?pretty' -d '
	> {
	>     "query" : {
	>         "bool" : {
	>             "should" : [
	>                 {"match" : {"address" : "mill"}},
	>                 {"match" : {"address" : "lane"}}
	>             ]
	>         }
	>     }
	> }'
	布尔查询之must_not查询:
	curl -XPOST 'localhost:9200/bank/_search?pretty' -d '
	{
	    "query" : {
	        "bool" : {
	            "must_not" : [
	                {"match" : {"address" : "mill"}},
	                {"match" : {"address" : "lane"}}
	            ]
	        }
	    }
	}'
	布尔查询之"多条件组合查询":
	curl -XPOST 'localhost:9200/bank/_search?pretty' -d '
	> {
	>     "query" : {
	>         "bool" : {
	>             "must" : [
	>                 {"match" : {"age" : "40"}}
	>             ],
	>             "must_not" : [
	>                 {"match" : {"state" : "ID"}}
	>             ]
	>         }
	>     }
	> }'

	过滤器查询:
	curl -XPOST 'localhost:9200/bank/_search?pretty' -d '
	> {
	>     "query" : {
	>         "filtered" : {
	>             "query" : {
	>                 "match_all" : {}
	>             },
	>             "filter" : {
	>                 "range" : {
	>                     "balance" : {
	>                         "gte" : 20000,
	>                         "lte" : 30000
	>                     }
	>                 }
	>             }
	>         }
	>     }
	> }'

	聚合查询:
	curl -XPOST 'localhost:9200/bank/_search?pretty' -d '
	> {
	>     "size" : 0,
	>     "args" : {
	>         "group_by_state" : {
	>             "terms" : {
	>                 "field" : "state"
	>             }
	>         }
	>     }
	> }'

	全文搜索：
	GET /megacorp/employee/_search
	{
	    "query" : {
	        "match" : {
	            "about" : "rock climbing"
	        }
	    }
	}

	短语查询：
	GET /megacorp/employee/_search
	{
	    "query" : {
	        "match_phrase" : {
	            "about" : "rock climbing"
	        }
	    }
	}

	高亮查询：
	GET /megacorp/employee/_search
	{
	    "query" : {
	        "match_phrase" : {
	            "about" : "rock climbing"
	        }
	    },
	    "highlight": {
	        "fields" : {
	            "about" : {}
	        }
	    }
	}

Elasticsearch的聚合搜索功能很强大；
使用version字段实现“乐观锁”功能，以及“外部锁”的概念；
对于_mget查询，不存在的文档会在响应体中进行告知；
对于_bulk操作，每个单独的操作都是相互独立的，一个单独子操作的错误不会影响其他子操作；




调整linux操作系统的vm.max_map_count参数方法：
1.修改/etc/sysctl.conf文件，最后一行添加：vm.max_map_count=262144
2.执行sudo sysctl -p命令使之生效
3.OK；




阅读笔记：
*.保存文档时的路由算法：shard = hash(routing) % number_of_primary_shards
*.我们能够发送请求给集群中任意一个节点。每个节点都有能力处理任意请求。每个节点都知道任意文档所在的节点，所以也可以将请求转发到需要的节点。当我们发送请求，最好的做法是循环通过所有节点请求，这样可以平衡负载。
*.复制(replication)默认的值是sync。这将导致主分片得到复制分片的成功响应后才返回。你可以设置replication为async,写完主分片即刻返回，然后异步复制分片。
*.默认consistency=int( (primary + number_of_replicas) / 2 ) + 1，可以为one或all。
*.当分片副本不足时会怎样？Elasticsearch会等待更多的分片出现。默认等待一分钟。
*.文档能够从主分片或任意一个复制分片被检索。
*.当主分片转发更改给复制分片时，并不是转发更新请求，而是转发整个文档的新版本。
*.mget和bulk API与单独的文档类似，它把多文档请求拆成每个分片的单文档请求，然后转发每个参与的节点。routing 参数可以被docs中的每个文档设置。
*.bulk API还可以在最上层使用replication和consistency参数，routing参数则在每个请求的元数据中使用。
*.我们可以将一个JSON文档扔给Elasticsearch，也可以根据ID检索它们。但Elasticsearch真正强大之处在于可以从混乱的数据中找出有意义的信息——从大数据到全面的信息。
*.倒排索引由在文档中出现的唯一的单词列表，以及对于每个单词在文档中的位置组成。
*.为了创建倒排索引，我们首先切分每个文档的content字段为单独的单词（我们把它们叫做词(terms)或者表征(tokens)）
*.索引文本和查询字符串都要标准化为相同的形式。这个标记化和标准化的过程叫做分词(analysis)
*.当我们索引(index)一个文档，全文字段会被分析为单独的词来创建倒排索引。不过，当我们在全文字段搜索(search)时，我们要让查询字符串经过同样的分析流程处理，以确保这些词在索引中存在。
*.查询全文(full text)字段时，查询将使用相同的分析器来分析查询字符串，以产生正确的词列表；查询一个确切值(exact value)字段时，查询将不分析查询字符串。
*.为了能够把日期字段处理成日期，把数字字段处理成数字，把字符串字段处理成全文本（Full-text）或精确的字符串值，Elasticsearch需要知道每个字段里面都包含了什么类型。这些类型和字段的信息存储（包含）在映射（mapping）中。
*.索引中每个文档都有一个类型(type)。 每个类型拥有自己的映射(mapping)或者模式定义(schema definition)。一个映射定义了字段类型，每个字段的数据类型，以及字段被Elasticsearch处理的方式。映射还用于设置关联到类型上的元数据。
*.可以使用_mapping后缀来查看Elasticsearch中的映射，例如：GET /gb/_mapping/tweet，这些映射是Elasticsearch在创建索引时动态生成的。
*.
	analyzed	首先分析这个字符串，然后索引。换言之，以全文形式索引此字段。
	not_analyzed	索引这个字段，使之可以被搜索，但是索引内容和指定值一样。不分析此字段。
	no	不索引这个字段。这个字段不能为搜索到。
	string类型字段默认值是analyzed。如果我们想映射字段为确切值，我们需要设置它为not_analyzed
*.Elasticsearch在一个简单的JSON接口中用结构化查询来展现Lucene绝大多数能力。 你应当在你的产品中采用这种方式进行查询。它使得你的查询更加灵活，精准，易于阅读并且易于debug。
*.我们可以使用两种结构化语句： 结构化查询（Query DSL）和结构化过滤（Filter DSL）。 查询与过滤语句非常相似，但是它们由于使用目的不同而稍有差异。
*.一条过滤语句会询问每个文档的字段值是否包含着特定值；一条查询语句与过滤语句相似，但问法不同：查询语句会询问每个文档的字段值与特定值的匹配程度如何。
*.原则上来说，使用查询语句做全文本搜索或其他需要进行相关性评分的时候，剩下的全部用过滤语句。过滤语句的目的就是缩小匹配的文档结果集，所以需要仔细检查过滤条件。
*.查询语句可以包含过滤子句，反之亦然。
*.可以验证一条查询语句是否合法。GET /gb/tweet/_validate/query?explain
*.计算 _score 是比较消耗性能的, 而且通常主要用作排序 -- 我们不是用相关性进行排序的时候，就不需要统计其相关性。 如果你想强制计算其相关性，可以设置track_scores为 true。
*.可以指定某个字段值用来作排序，这样就不再计算_score分值了（_score 和 max_score 字段都为 null）；字段值默认以顺序排列，而 _score 默认以倒序排列。
*.可以根据多个字段，进行多级排序；
*.对于多值字段的排序，对于数字和日期，你可以从多个值中取出一个来进行排序，你可以使用min, max, avg 或 sum这些模式。
*.排序问题就是：默认用_score进行倒序排序，如果指定了一个字段则就以这个字段进行排序（默认正序），同时不再计算_score分值了；
*.排序时使用完整的not_analyzed字符串（原字符串），全文本搜索的时候还必须使用被 analyzed 标记的字段，要达到这个目标，只需要：改变索引(index)的mapping即可，方法是在所有核心字段类型上，使用通用参数 fields对mapping进行修改。在给数据重建索引后，我们既可以使用 tweet 字段进行全文本搜索，也可以用tweet.raw字段进行排序。
*.对 analyzed 字段进行强制排序会消耗大量内存。
*.查询语句会为每个文档添加一个_score字段。fuzzy 查询会计算与关键词的拼写相似程度，terms查询会计算找到的内容与关键词组成部分匹配的百分比，但是一般意义上我们说的全文本搜索是指计算内容与关键词的类似程度。
*.ElasticSearch的相似度算法被定义为 TF/IDF，即检索词频率/反向文档频率
*.1.检索词频率::检索词在该字段出现的频率？出现频率越高，相关性也越高。 字段中出现过5次要比只出现过1次的相关性高。
*.2.反向文档频率::每个检索词在索引中出现的频率？频率越高，相关性越低。 检索词出现在多数文档中会比出现在少数文档中的权重更低，即检验一个检索词在文档中的普遍重要性。
*.3.字段长度准则::字段的长度是多少？长度越长，相关性越低。检索词出现在一个短的 title 要比同样的词出现在一个长的 content 字段的相关性高。
*.相关性并不只是全文本检索的专利。也适用于yes|no的子句，匹配的子句越多，相关性评分越高。如果多条查询子句被合并为一条复合查询语句，比如 bool 查询，则每个查询子句计算得出的评分会被合并到总的相关性评分中。
*.在每个查询语句中都有一个explain参数，将 explain 设为 true 就可以得到更详细的信息。explain 参数可以让返回结果添加一个 _score 评分的得来依据。
*.词频率和文档频率是在每个分片中计算出来的，而不是每个索引中
*.JSON形式的explain描述是难以阅读的，但是转成 YAML 会好很多，只需要在参数中加上 format=yaml
*.当explain选项加到某一文档上时，它会告诉你为何这个文档会被匹配，以及一个文档为何没有被匹配。比如请求路径/index/type/id/_explain可看到解释
*.为了提高排序效率，ElasticSearch 会将所有字段的值加载到内存中，这就叫做"数据字段"。
*.ElasticSearch将所有字段数据加载到内存中并不是匹配到的那部分数据，而是索引下所有文档中的值，包括所有类型。???这是因为：：将所有字段数据加载到内存中是因为从硬盘反向倒排索引是非常缓慢的。尽管你这次请求需要的是某些文档中的部分数据， 但你下个请求却需要另外的数据，所以将所有字段数据一次性加载到内存中是十分必要的。毫无疑问，这会消耗掉很多内存，值得庆幸的是，内存不足是可以通过横向扩展解决的，我们可以增加更多的节点到集群。
*.搜索的执行过程分两个阶段，称为查询然后取回（query then fetch）。
*.在初始化查询阶段（query phase），查询被向索引中的每个分片副本（原本或副本）广播。每个分片在本地执行搜索并且建立了匹配document的优先队列。
*.一个优先队列（priority queue is）只是一个存有前n个（top-n）匹配document的有序列表。这个优先队列的大小由分页参数from和size决定。
*.当一个搜索请求被发送到一个节点Node，这个节点就变成了协调节点。这个节点的工作是向所有相关的分片广播搜索请求并且把它们的响应整合成一个全局的有序结果集。这个结果集会被返回给客户端。对于后续请求，协调节点会轮询所有的分片副本以分摊负载。
*.每一个分片在本地执行查询和建立一个长度为from+size的有序优先队列——这个长度意味着它自己的结果数量就足够满足全局的请求要求。分片返回一个轻量级的结果列表给协调节点。只包含documentID值和排序需要用到的值，例如_score。协调节点将这些分片级的结果合并到自己的有序优先队列里。这个就代表了最终的全局有序结果集。
*.整个过程类似于归并排序算法，先分组排序再归并到一起，对于这种分布式场景非常适用。
*.协调节点先决定哪些document是实际（actually）需要取回的，然后为每个持有相关document的分片建立多点get请求然后发送请求到处理查询阶段的分片副本。
*.preference参数允许你控制使用哪个分片或节点来处理搜索请求。
*.按照timestamp字段来对你的结果排序，并且有两个document有相同的timestamp。由于搜索请求是在所有有效的分片副本间轮询的，这两个document可能在原始分片里是一种顺序，在副本分片里是另一种顺序。：：这就是所谓的“结果震荡”。避免这个问题的方法是对于同一个用户总是使用同一个分片。
*.timeout参数告诉协调节点最多等待多久，就可以放弃等待而将已有结果返回。返回部分结果总比什么都没有好。
*.虽然query_then_fetch是默认的搜索类型，但也可以根据特定目的指定其它的搜索类型(GET /_search?search_type=count)，
*.一个滚屏搜索允许我们做一个初始阶段搜索并且持续批量从Elasticsearch里拉取结果直到没有结果剩下。这有点像传统数据库里的cursors（游标）。滚屏搜索会及时制作快照。这个快照不会包含任何在初始阶段搜索请求后对index做的修改。它通过将旧的数据文件保存在手边，所以可以保护index的样子看起来像搜索开始时的样子。
*.采用scan（扫描）搜索模式。扫描模式让Elasticsearch不排序，只要分片里还有结果可以返回，就返回一批结果。
*.开启扫描搜索和滚屏取回：：GET /old_index/_search?search_type=scan&scroll=1m，请求的应答没有包含任何命中的结果，但是包含了一个Base-64编码的_scroll_id（滚屏id）字符串。现在我们可以将_scroll_id 传递给_search/scroll末端来获取第一批结果：
GET /_search/scroll?scroll=1mc2Nhbjs1OzExODpRNV9aY1VyUVM4U0N...
*.当扫描时，size被应用到每一个分片上，所以我们在每个批次里最多可获得size * number_of_primary_shards（size*主分片数）个document。
*.滚屏请求也会返回一个新的_scroll_id。每次做下一个滚屏请求时，必须传递前一次请求返回的_scroll_id。
*.可以这样简单总结记忆：：“扫描搜索”就是不排序搜索，“滚屏获取”就是类似于关系型数据库中的“游标”获取。
*.通过简单的添加一个文档的方式创建了一个索引。这个索引使用默认设置，新的属性通过动态映射添加到分类中。我们当然可以在创建索引时进行更多控制，比如：：确保索引被创建在适当数量的分片上，在索引数据之前设置好分析器和类型映射等等。手动创建索引，在请求中加入所有设置和类型映射。
*.可以通过在 config/elasticsearch.yml 中添加下面的配置来防止自动创建索引：：action.auto_create_index: false
*.Elasticsearch 提供了优化好的默认配置。除非你明白这些配置的行为和为什么要这么做，请不要修改这些配置。
*.两个最重要的设置：
	1.number_of_shards
	定义一个索引的主分片个数，默认值是 `5`。这个配置在索引创建后不能修改。
	2.number_of_replicas
	每个主分片的复制分片个数，默认是 `1`。这个配置可以随时在活跃的索引上修改。
*.分析器，用于将全文字符串转换为适合搜索的倒排索引。
*.默认情况下，停用词过滤器是被禁用的。如需启用它，你可以通过创建一个基于 standard 分析器的自定义分析器，并且设置 stopwords 参数。可以提供一个停用词列表，或者使用一个特定语言的预定停用词列表。
*.可以通过在配置文件中组合字符过滤器，分词器和标记过滤器，来满足特定数据的需求。
*.分析器：：是三个顺序执行的组件的结合（1.字符过滤器，2.分词器，3.标记过滤器）。
*.1.字符过滤器的作用是让字符串在被分词前变得更加“整洁”，一个分析器可能包含零到多个字符过滤器。
*.2.分词器将字符串分割成单独的词（terms）或标记（tokens），一个分析器 必须 包含一个分词器。
*.3.标记过滤器可能修改，添加或删除标记，分词结果的 标记流 会根据各自的情况，传递给特定的标记过滤器。
*.我们需要告诉Elasticsearch在哪里使用分析器，分析器才会起作用。
*.类型和映射：：类型在Elasticsearch中表示一组相似的文档，类型由一个名称（比如 user 或 blogpost）和一个类似于关系型数据库表结构的映射组成；映射描述了文档中可能包含的每个字段的属性，数据类型（比如 string, integer 或 date），和是否这些字段需要被Lucene索引或储存。
*.因为Lucene没有文档类型的概念，每个文档的类型名被储存在一个叫_type的元数据字段上。当我们搜索一种特殊类型的文档时，Elasticsearch简单的通过_type字段来过滤出这些文档。
*.Lucene中同样没有映射的概念。映射是Elasticsearch将复杂JSON文档映射成Lucene需要的扁平化数据的方式。
*.类型陷阱：事实上不同类型的文档可以被加到同一个索引里带来了一些预想不到的困难。想象一下我们的索引中有两种类型：blog_en 表示英语版的博客，blog_es 表示西班牙语版的博客。两种类型都有 title 字段，但是其中一种类型使用 english 分析器，另一种使用 spanish 分析器。这种情况下使用title查询就会带来问题。我们可以通过给字段取不同的名字来避免这种错误或者在查询中明确包含各自的类型名来解决这个问题。
*.映射的最高一层被称为 根对象，它可能包含下面几项：1.一个 properties 节点，列出了文档中可能包含的每个字段的映射,2.多个元数据字段，每一个都以下划线开头，例如 _type, _id 和 _source,3.设置项，控制如何动态处理新的字段，例如 analyzer, dynamic_date_formats 和 dynamic_templates。4.其他设置，可以同时应用在根对象和其他 object 类型的字段上，例如 enabled, dynamic 和 include_in_all。
*.文档字段和属性的三个最重要的设置：1.type： 字段的数据类型，例如 string 和 date,2.index： 字段是否应当被当成全文来搜索（analyzed），或被当成一个准确的值（not_analyzed），还是完全不可被搜索（no）,3.analyzer： 确定在索引和或搜索时全文字段使用的 分析器。
*.默认情况下，Elasticsearch 用 JSON 字符串来表示文档主体并保存在 _source 字段中。像其他保存的字段一样，_source 字段也会在写入硬盘前压缩。
*.即便将文档主体保存在_source字段中对很多功能很重要，但存储_source字段还是要占用硬盘空间的。假如这些功能对你不重要，可以通过定义映射禁用_source字段。
*.在搜索请求中你可以通过限定 _source 字段来请求指定字段，这些字段会从 _source 中提取出来，而不是返回整个 _source 字段。
*.在Elasticsearch中，完整的文档已经被保存在_source字段中了，通常最好的办法会是使用_source参数来过滤你需要的字段。
*.介绍了_all字段：一个包含了所有其他字段值的特殊字符串字段。query_string在没有指定字段时默认用_all字段查询。作用在_all字段上的搜索是一种简单粗暴的搜索方式。通过查询独立的字段，你能更灵活，强大和精准的控制搜索结果，提高相关性。
*.【相关性算法】考虑的一个最重要的原则是字段的长度：字段越短，就越重要。在较短的 title 字段中的短语会比较长的 content 字段中的短语显得更重要。而字段间的这种差异在 _all 字段中就不会出现
*.如果决定不再使用 _all 字段，可以通过映射禁用它
*.通过_mapping中的include_in_all选项可以控制字段是否要被包含在_all字段中，默认值是true。在一个对象上设置include_in_all可以修改这个对象所有字段的默认行为。
*.相对于完全禁用_all字段，你可以先默认禁用include_in_all选项，而在选定字段上启用include_in_all。
*.谨记_all字段仅仅是一个经过分析的string字段。它使用默认的分析器来分析它的值，而不管这值本来所在的字段指定的分析器。而且像所有string类型字段一样，你可以配置_all字段使用的分析器
*.文档唯一标识由四个元数据字段组成：1._index：文档所在的索引;2._type：文档的类型名;3._id：文档的字符串ID;4._uid：_type和_id连接成的type#id
*.默认情况下，_uid 是被保存（可取回）和索引（可搜索）的。_type 字段被索引但是没有保存，_id 和 _index 字段则既没有索引也没有储存，它们并不是真实存在的。
*.尽管如此，你仍然可以像真实字段一样查询 _id 字段。Elasticsearch 使用 _uid 字段来追溯 _id。虽然你可以修改这些字段的 index 和 store 设置，但是基本上不需要这么做。
*._id字段有一个你可能用得到的设置：path设置告诉Elasticsearch它需要从文档本身的哪个字段中生成_id；警告：虽然这样很方便，但是注意它对bulk请求有个轻微的性能影响。处理请求的节点将不能仅靠解析元数据行来决定将请求分配给哪一个分片，而需要解析整个文档主体。
*.当 Elasticsearch 处理一个位置的字段时，它通过【动态映射】来确定字段的数据类型且自动将该字段加到类型映射中。可以通过 dynamic 设置来控制这些行为，它接受下面几个选项：1.true：自动添加字段（默认）；2.false：忽略字段；3.strict：当遇到未知字段时抛出异常。dynamic 设置可以用在根对象或任何 object 对象上。你可以在根对象上将 dynamic 默认设置为 strict，而在特定内部对象上启用它
*.将 dynamic 设置成 false 完全不会修改 _source 字段的内容。_source 将仍旧保持你索引时的完整 JSON 文档。然而，没有被添加到映射的未知字段将不可被搜索。
*.可以通过设置来自定义动态索引规则。
*.当 Elasticsearch 遇到一个新的字符串字段时，它会检测这个字段是否包含一个可识别的日期，比如 2014-01-01。如果它看起来像一个日期，这个字段会被作为 date 类型添加，否则，它会被作为 string 类型添加。
*.日期检测可以通过在根对象上设置 date_detection 为 false 来关闭
*.提示：Elasticsearch 判断字符串为日期的规则可以通过 dynamic_date_formats 配置 来修改。
*.使用dynamic_templates，你可以完全控制新字段的映射，你设置可以通过字段名或数据类型应用一个完全不同的映射。
*.每个模板都有一个名字用于描述这个模板的用途，一个 mapping 字段用于指明这个映射怎么使用，和至少一个参数（例如 match）来定义这个模板适用于哪个字段。
*.模板按照顺序来检测，第一个匹配的模板会被启用。
*.用_default_映射来指定公用设置会更加方便，而不是每次创建新的类型时重复操作。_default映射像新类型的模板。所有在_default_映射之后的类型将包含所有的默认设置，除非在自己的类型映射中明确覆盖这些配置。
*._default_映射也是定义索引级别的动态模板的好地方。
*.修改已存在的数据最简单的方法是重新索引：创建一个新配置好的索引，然后将所有的文档从旧的索引复制到新的上。_source 字段的一个最大的好处是你已经在 Elasticsearch 中有了完整的文档，你不再需要从数据库中重建你的索引，这样通常会比较慢。为了更高效的索引旧索引中的文档，使用【scan-scoll】来批量读取旧索引的文档，然后将通过【bulk API】来将它们推送给新的索引。
*.你可以在同一时间执行多个重新索引的任务，可以将重建大索引的任务通过日期或时间戳字段拆分成较小的任务
*.假如你继续在旧索引上做修改，你可能想确保新增的文档被加到了新的索引中。这可以通过重新运行重建索引程序来完成，但是只要过滤出上次执行后新增的文档就行了。
*.重新索引过程中的问题是必须更新你的应用，来使用另一个索引名。索引别名正是用来解决这个问题的！
*.索引 别名 就像一个快捷方式或软连接，可以指向一个或多个索引，也可以给任何需要索引名的 API 使用。别名带给我们极大的灵活性，可以做到在零停机时间内从旧的索引切换到新的索引。
*.有两种管理别名的途径：_alias 用于单个操作，_aliases 用于原子化多个操作。
*.事实上，my_index 是一个指向当前真实索引的别名。真实的索引名将包含一个版本号：my_index_v1, my_index_v2 等等。
*.创建一个索引 my_index_v1，然后将别名 my_index 指向它：1.PUT /my_index_v1；2.PUT /my_index_v1/_alias/my_index；可以检测一个别名指向哪个索引：GET /*/_alias/my_index，或哪些别名指向这个索引：GET /my_index_v1/_alias/*
*.【重新索引】完成后，我们就将别名指向新的索引。
*.别名可以指向多个索引，所以我们需要在新索引中添加别名的同时从旧索引中删除它。这个操作需要原子化，所以我们需要用 _aliases 操作：
	POST /_aliases
	{
	    "actions": [
	        { "remove": { "index": "my_index_v1", "alias": "my_index" }},
	        { "add":    { "index": "my_index_v2", "alias": "my_index" }}
	    ]
	}
*.即使你认为现在的索引设计已经是完美的了，但当你的应用在生产环境中使用时，还是有可能在今后有一些改变的。所以请做好准备：在应用中使用别名而不是索引。然后你就可以在任何时候重建索引。别名的开销很小，应当广泛使用。
*.支持一个字段多个值的最佳数据结构是倒排索引。倒排索引包含了出现在所有文档中唯一的值或词的有序列表，以及每个词所属的文档列表。
*.倒排索引存储了比包含了一个特定term的文档列表多地多的信息。它可能存储包含每个term的文档数量，一个term出现在指定文档中的频次，每个文档中term的顺序，每个文档的长度，所有文档的平均长度，等等。这些统计信息让Elasticsearch知道哪些term更重要，哪些文档更重要，也就是相关性。
*.需要意识到，为了实现倒排索引预期的功能，它必须要知道集合中所有的文档。
*.在全文检索的早些时候，会为整个文档集合建立一个大索引，并且写入磁盘。只有新的索引准备好了，它就会替代旧的索引，最近的修改才可以被检索。
*.写入磁盘的倒排索引是不可变的，它有如下好处：不需要锁。一旦索引被读入内存，它就一直在那儿。写入单个大的倒排索引，可以压缩数据。
*.如何在保持不可变好处的同时更新倒排索引。答案是，使用多个索引。不是重写整个倒排索引，而是增加额外的索引反映最近的变化。每个倒排索引都可以按顺序查询，从最老的开始，最后把结果聚合。
*.在Lucene中的两个总要概念：：1.段（Segment）：一个完整的倒排索引；2.提交点（Commit Point）:包含段的文件；
*.Elasticsearch底层依赖的Lucene，引入了per-segment search的概念。一个段(segment)是有完整功能的倒排索引，但是现在Lucene中的索引指的是段的集合，再加上提交点(commit point，包括所有段的文件)，在Lucene中新的文档在被写入磁盘的段之前，首先被写入内存区的索引缓存。
*.需要说明，Lucene索引是Elasticsearch中的分片，Elasticsearch中的索引是分片的集合。当Elasticsearch搜索索引时，它发送查询请求给该索引下的所有分片，然后过滤这些结果，聚合成全局的结果。
*.在Lucene中，一个per-segment search如下工作:	
	1.新的文档首先被写入内存区的索引缓存。
	2. 一段时间以后，这些buffer被提交：
		2.1.一个新的段——额外的倒排索引——写入磁盘。
		2.2.新的提交点写入磁盘，包括新段的名称。
		2.3.磁盘是fsync’ed(文件同步)——所有写操作等待文件系统缓存同步到磁盘，确保它们可以被物理写入。
	3.新段被打开，它包含的文档可以被检索
	4.内存的缓存被清除，等待接受新的文档。
*.当一个查询请求被接受，所有段依次查询。所有段上的Term统计信息被聚合，确保每个term和文档的相关性被正确计算。通过这种方式，新的文档以较小的代价加入索引。
*.当一个文档被删除，它实际上只是在.del文件中被标记为删除，依然可以匹配查询，但是最终返回之前会被从结果中删除。
*.当一个文档被更新，旧版本的文档被标记为删除，新版本的文档在新的段中索引。也许该文档的不同版本都会匹配一个查询，但是更老版本会从结果中删除。
*.Lucene允许新段写入打开，好让它们包括的文档可搜索，而不用执行一次全量提交。这是比提交更轻量的过程，可以经常操作，而不会影响性能。
*.在Elesticsearch中，这种写入打开一个新段的轻量级过程，叫做refresh。默认情况下，每个分片每秒自动刷新一次。这就是为什么说Elasticsearch是近实时的搜索了：文档的改动不会立即被搜索，但是会在一秒内可见。
*.如果新用户感到困惑：他们索引了个文档，尝试搜索它，但是搜不到。解决办法就是执行一次手动刷新，通过API:POST /_refresh 或POST /blogs/_refresh
*.虽然刷新比提交更轻量，但是它依然有消耗。手工刷新在开发测试的时候有用，但是不要在生产环境中每写一次就执行刷新，这会影响性能。相反，你的应用需要意识到ES近实时搜索的本质，并且容忍它。
*.可以通过修改配置项refresh_interval改变刷新的频率
*.自动刷新间隔refresh_interval可以在存在的索引上动态更新。你在创建大索引的时候可以关闭自动刷新，在要使用索引的时候再打开它。
*.每秒定时刷新 + 事务日志 + 全提交，确保了elasticsearch又快又永久存储。
*.事务日志记录了所有没有flush到硬盘的所有操作。当故障重启后，ES会用最近一次提交点从硬盘恢复所有已知的段，并且从事务日志里恢复所有的操作。
*.事务日志还用来提供实时的CRUD操作。当你尝试用ID进行CRUD时，它在检索相关段内的文档前会首先检查日志最新的改动。这意味着ES可以实时地获取文档的最新版本。
*.在ES中，进行一次刷新的操作叫做refresh。分片默认每1秒进行一次，可以通过设置项refresh_interval进行改变。
*.在ES中，进行一次提交并删除事务日志的操作叫做flush。分片每30分钟，或事务日志过大会进行一次flush操作。
*.你很少需要手动flush，通常自动的就够了。当你要重启或关闭一个索引，flush该索引是很有用的，当ES尝试恢复或者重新打开一个索引时，它必须重放所有事务日志中的操作，所以日志越小，恢复速度越快。
*.有太多的段是一个问题，每个段消费文件句柄，内存，cpu资源。更重要的是，每次搜索请求都需要依次检查每个段。段越多，查询越慢。ES通过后台合并段解决这个问题。小段被合并成大段，再合并成更大的段。
*.合并大的段会消耗很多IO和CPU，如果不检查会影响到搜素性能。默认情况下，ES会限制合并过程，这样搜索就可以有足够的资源进行。
*.optimize API最好描述为强制合并段API。它强制分片合并段以达到指定max_num_segments参数。这是为了减少段的数量（通常为1）达到提高搜索性能的目的。
*.不要在动态的索引（正在活跃更新）上使用optimize API。后台的合并处理已经做的很好了，优化命令会阻碍它的工作。不要干涉！只是在特定的环境下，optimize API是有用的。
*.结构化搜索是指查询包含内部结构的数据。日期，时间，和数字都是结构化的：它们有明确的格式给你执行逻辑操作。一般包括比较数字或日期的范围，或确定两个值哪个大。
*.通过结构化搜索，你的查询结果始终是 是或非；是否应该属于集合。结构化搜索不关心文档的相关性或分数，它只是简单的包含或排除文档。
*.结构化搜索没有 更匹配 的概念。
*.对于准确值，你需要使用过滤器。过滤器的重要性在于它们非常的快。它们不计算相关性（避过所有计分阶段）而且很容易被缓存。请先记住尽可能多的使用过滤器。
*.在Elasticsearch DSL中，我们使用term过滤器来实现同样的事。term过滤器会查找我们设定的准确值。term过滤器本身很简单，它接受一个字段名和我们希望查找的值
*.单独使用term过滤器本身并不能起作用。像在【查询 DSL】中介绍的一样，搜索API需要得到一个查询语句，而不是一个过滤器。为了使用term过滤器，我们需要将它包含在一个过滤查询语句中，示例如下：
	GET /my_store/products/_search
	{
	    "query" : {
	        "filtered" : { <1>
	            "query" : {
	                "match_all" : {} <2>
	            },
	            "filter" : {
	                "term" : { <3>
	                    "price" : 20
	                }
	            }
	        }
	    }
	}，注意term过滤器在filter分句中的位置。
*.过滤器不会执行计分和计算相关性，分值由match_all查询产生，所有文档一视同仁，所有每个结果的分值都是1。
*.当我们用XHDK-A-1293-#fJ3来查找时，得不到任何结果，因为这个标记不在我们的倒排索引中。相反，那里有四个标记。为了避免这种情况发生，我们需要通过设置这个字段为not_analyzed来告诉Elasticsearch它包含一个准确值。
*.当执行 filtered 查询时，filter会比query早执行。结果字节集会被传给query来跳过已经被排除的文档。这种过滤器提升性能的方式，查询更少的文档意味着更快的速度。
*.过滤多个值或字段，需要使用bool过滤器。这是以其他过滤器作为参数的组合过滤器，将它们结合成多种布尔组合。bool过滤器由三部分组成：
	{
	   "bool" : {
	      "must" :     [],
	      "should" :   [],
	      "must_not" : [],
	   }
	}
*.bool过滤器的每个部分都是可选的，例如，你可以只保留一个must分句，而且每个部分可以包含一到多个过滤器。
*.注意我们仍然需要用filtered查询来包裹所有条件。
*.可以在bool过滤器中嵌套bool过滤器，从而实现更复杂的布尔逻辑。
*.比起使用多个term过滤器，你可以用一个terms过滤器。terms过滤器是term过滤器的复数版本。
*.理解term和terms是包含操作，而不是相等操作，这点非常重要。因为term过滤器的工作过程是：它检查倒排索引中所有具有短语的文档，然后组成一个字节集。
*.假如你真的需要完全匹配这种行为，最好是通过添加另一个字段来实现。在这个字段中，你索引原字段包含值的个数。这将匹配只有一个search标签的文档，而不是匹配所有包含了search标签的文档。一旦你索引了标签个数，你可以构造一个bool过滤器来限制短语个数。
*.在Elasticsearch中有一个range过滤器，让你可以根据范围进行过滤，range过滤器既能包含也能排除范围。range过滤器也可以用于日期字段。当用于日期字段时，range过滤器支持日期数学操作，比如：now-1h，这个过滤器将始终能找出所有时间戳大于当前时间减1小时的文档。
*.日期计算也能用于实际的日期，而不是仅仅是一个像now一样的占位符。只要在日期后加上双竖线||，就能使用日期数学表达式了，比如：2014-01-01 00:00:00||+1M
*.日期计算是与日历相关的，所以它知道每个月的天数，每年的天数，等等。
*.range过滤器也可以用于字符串。字符串范围根据字典或字母顺序来计算。倒排索引中的短语按照字典顺序排序
*.为了在字符串上执行范围操作，Elasticsearch会在这个范围内的每个短语执行term操作。这比日期或数字的范围操作慢得多。
*.倒排索引是标记和包含它们的文档的一个简单列表。假如一个字段不存在，它就没有任何标记，也就意味着它无法被倒排索引的数据结构表达出来。本质上来说，null，[]（空数组）和 [null] 是相等的。它们都不存在于倒排索引中！Elasticsearch有一些工具来处理空值或缺失的字段。
*.工具箱中的第一个利器是exists过滤器，这个过滤器将返回任何包含这个字段的文档
*.missing过滤器本质上是exists的反义词：它返回没有特定字段值的文档
*.默认行为无法区分一个字段是没有值，还是被设置为null，幸运的是，我们可以将明确的null值用我们选择的占位符来代替。
*.当指定字符串，数字，布尔值或日期字段的映射时，你可以设置一个null_value来处理明确的null值。没有值的字段仍将被排除在倒排索引外。
*.exists和missing过滤器同样能在内联对象上工作，而不仅仅是核心类型。
*.过滤器工作的核心是用一个字节集来表示哪些文档符合这个过滤器。Elasticsearch主动缓存了这些字节集留作以后使用。一旦缓存后，当遇到相同的过滤时，这些字节集就可以被重用，而不需要重新运算整个过滤。
*.缓存的字节集很“聪明”：他们会增量更新。你索引中添加了新的文档，只有这些新文档需要被添加到已存的字节集中，而不是一遍遍重新计算整个缓存的过滤器。过滤器和整个系统的其他部分一样是实时的，你不需要关心缓存的过期时间。
*.每个过滤器都被独立计算和缓存，而不管它们在哪里使用。如果两个不同的查询使用相同的过滤器，则会使用相同的字节集。同样，如果一个查询在多处使用同样的过滤器，只有一个字节集会被计算和重用。
*.大部分直接处理字段的枝叶过滤器（例如term）会被缓存，而像bool这类的组合过滤器则不会被缓存。枝叶过滤器需要在硬盘中检索倒排索引，所以缓存它们是有意义的。另一方面来说，组合过滤器使用快捷的字节逻辑来组合它们内部条件生成的字节集结果，所以每次重新计算它们也是很高效的。
*.对于有些过滤器，因为重用性很低，所以默认是不被缓存的，如脚本过滤器、GEO过滤器、日期范围过滤器（因为精确到毫秒，例如 "now-1h"；但当now被取整时，默认是被缓存的，例如“now/d”取最近一天）
*.可以通过_cache标记来覆盖几乎所有过滤器的默认缓存策略
*.在bool条件中过滤器的顺序对性能有很大的影响。更详细的过滤条件应该被放置在其他过滤器之前，以便在更早的排除更多的文档。假如条件A匹配1000万个文档，而B只匹配100个文档，那么需要将B放在A前面。
*.缓存的过滤器非常快，所以它们需要被放在不能缓存的过滤器之前。
*.可以通过组合一个缓存的过滤器来让now方法的过滤查询变得更有效率：我们可以添加一个含固定时间的过滤器来排除掉这个月的大部分数据，例如昨晚凌晨
*.全文检索：怎样对全文字段(full-text fields)进行检索以找到相关度最高的文档。
*.全文检索最重要的两个方面是：：1.相关度(Relevance)：根据文档与查询的相关程度对结果集进行排序的能力。相关度可以使用TF/IDF、地理位置相近程度、模糊相似度或其他算法计算；2.分析(Analysis)：将一段文本转换为一组唯一的、标准化了的标记(token)，用以(a)创建倒排索引，(b)查询倒排索引。
*.一旦提到相关度和分析，指的都是查询(queries)而非过滤器(filters)。
*.虽然所有的查询都会进行相关度计算，但不是所有的查询都有分析阶段。而且像bool或function_score这样的查询并不在文本字段执行。
*.像term或fuzzy一类的查询是低级查询，它们没有分析阶段。这些查询在单一的短语上执行。例如对单词'Foo'的term查询会在倒排索引里精确地查找'Foo'这个词，而不会匹配短语的其它变形，如foo或FOO，并对每个包含这个单词的文档计算TF/IDF相关度'_score'。不管短语怎样被加入索引，都只匹配倒排索引里的准确值。
*.像match和query_string这样的查询是高级查询，它们会对字段进行分析：1.如果检索一个'date'或'integer'字段，它们会把查询语句作为日期或者整数格式数据。2.如果检索一个准确值('not_analyzed')字符串字段，它们会把整个查询语句作为一个短语。3.如果检索一个全文('analyzed')字段，查询会先用适当的解析器解析查询语句，产生需要查询的短语列表。然后对列表中的每个短语执行低级查询，合并查询结果，得到最终的文档相关度。
*.我们很少需要直接使用基于短语的查询。通常我们会想要检索全文，而不是单独的短语，使用高级的全文检索会更简单（全文检索内部最终还是使用基于短语的查询）。
*.如果确实要查询一个准确值字段('not_analyzed')，需要考虑使用查询还是过滤器。单一短语的查询通常相当于是/否问题，用过滤器可以更好的描述这类查询，并且过滤器缓存可以提升性能。
*.不管你搜索什么内容，match查询是你首先需要接触的查询。它是一个高级查询，意味着match查询知道如何更好的处理全文检索和准确值检索。这也就是说，match查询的一个主要用途是进行全文搜索。
*.因为要查询的目标字段是一个字符串(analyzed)，所以用来查询的字符串也需要被分析(analyzed)。
*.简单匹配查询的执行过程：：1.检查目标字段类型，如果是字符串（alalyzed），则查询字符串也需要被分析（analyzed）；2.分析查询字符串，产生token或term；3.在倒排索引中搜索token或term，返回文档编号；4.根据一定规则给文档打分。
*.当match查询需要查询两个关键词："brown"和"dog"，在内部会执行两个term查询并综合二者的结果得到最终的结果。match的实现方式是将两个term查询放入一个bool查询。重要的一点是，'title'字段包含至少一个查询关键字的文档都被认为是符合查询条件的，匹配的单词数越多，文档的相关度越高。
*.匹配包含任意个数查询关键字的文档可能会得到一些看似不相关的结果，这是一种霰弹策略(shotgun approach)。然而我们可能想得到包含所有查询关键字的文档。换句话说，我们想得到的是匹配'brown AND dog'的文档，而非'brown OR dog'。
*.match查询接受一个'operator'参数，默认值为or。如果要求所有查询关键字都匹配，可以更改参数值为and。
*.match查询有'minimum_should_match'参数，参数值表示被视为相关的文档必须匹配的关键词个数。参数值可以设为整数，也可以设置为百分数。因为不能提前确定用户输入的查询关键词个数，使用百分数也很合理。不论参数值为多少，进入结果集的文档至少应匹配一个关键词。
*.布尔查询会做更精细的判断，他们不仅决定一个文档是否要添加到结果集，而且还要计算文档的相关性（relevant）。
*.像布尔过滤器一样, 布尔查询接受多个用must, must_not, 和should的查询子句。
*.should查询子句与should过滤器的区别：：当should中包含两个（及以上）子句时，区别就体现出来了：一个文档不需要同时包含brown和dog，但如果同时有这两个词，这个文档的相关性就更高。
*.组合查询得分计算：：布尔查询通过把所有符合must和should的子句得分加起来，然后除以must和should子句的总数，即为每个文档计算相关性得分。must_not子句并不影响得分，他们存在的意义是排除已经被包含的文档。
*.组合查询精度控制：：所有的must子句必须匹配, 并且所有的must_not子句必须不匹配。当有must子句时，不需要匹配任何should子句；当没有must子句时，就必须至少匹配一个should子句。我们也可以通过minimum_should_match参数控制多少should子句需要被匹配，这个参数可以是正整数，也可以是百分比。
*.match匹配查询与bool查询在很多情况下都是可以互相等价的；当然，我们通常写这些查询类型的时候还是使用match查询的，但是理解match查询在内部是怎么工作的可以让你在任何你需要使用的时候更加得心应手。有些情况仅仅使用一个match查询是不够的，比如给某些查询词更高的权重。
*.实际上，bool查询并不仅仅是组合多个简单的一个词的match查询。他可以组合任何其他查询，包括bool查询。bool查询通常会通过组合几个不同查询的得分为每个文档调整相关性得分。
*.假设我们想查找关于"full-text search"的文档，但是我们又想给涉及到“Elasticsearch”或者“Lucene”的文档更高的权重。我们的用意是想涉及到"Elasticsearch"或者"Lucene"的文档的相关性得分会比那些没有涉及到的文档的得分要高，也就是说这些文档会出现在结果集更靠前的位置。
*.匹配的should子句越多，文档的相关性就越强。
*.我们可以在任何查询子句中指定一个boost值来控制相对权重，默认值为1。一个大于1的boost值可以提高查询子句的相对权重。
*.match的参数boost用于提高子句的相对权重（boost值大于1）或者降低子句的相对权重（boost值在0-1之间），但是提高和降低并非是线性的。换句话说，boost值为2并不能够使结果变成两部的得分。
*.参数boost值被使用了以后新的得分是标准的。每个查询类型都会有一个独有的标准算法。简单的概括一下，一个更大的boost值可以得到一个更高的得分。
*.如果你自己实现了没有基于TF/IDF的得分模型，但是你想得到更多的对于提高得分过程的控制，你可以使用function_score查询来调整一个文档的boost值而不用通过标准的步骤。
*.查询只能查找在倒排索引中出现的词，所以确保在文档索引的时候以及字符串查询的时候使用同一个分析器是很重要的，为了查询的词能够在倒排索引中匹配到。
*.尽管说文档中每个字段的分析器是已经定好的。但是每个字段可以有不同的分析器，通过给每个字段配置一个指定的分析器或者直接使用类型，索引，或节点上的默认分析器。在索引的时候，一个字段的值会被配置的或者默认的分析器分析。
*.像match查询一样的高级别的查询可以知道字段的映射并且能够在被查询的字段上使用正确的分析器。match查询为每个字段使用合适的分析器用来确保在那个字段上可以用正确的格式查询这个词。
*.分析器可以在好几个地方设置。Elasticsearch会查找每个级别直到找到它可以使用的分析器。在创建索引的时候，Elasticsearch会按照一定的顺序查找分析器。
*.查找索引的时候，Elasticsearch查找分析器的顺序与创建索引时稍微有点不一样。
*.有时候，在创建索引与查询索引的时候使用不同的分析器也是有意义的，比如在创建索引的时候想要索引同义词。为了满足这种差异，Elasticsearch也支持index_analyzer和search_analyzer参数，并且分析器被命名为default_index和default_search。
*.可以指定分析器的地方实在是太多了，
*.通常见到的情形是，在同一个集群中运行着好几个不同的应用。每一个索引都需要是独立的，并且可以被独立的配置。
*.保持Elasticsearch持续运行并通过API来管理索引的设置是一个更好的方法。
*.大多数时间，你可以预先知道文档会包含哪些字段。最简单的方法是在你创建索引或者添加类型映射的时候为每一个全文检索字段设置分析器。虽然这个方法有点啰嗦，但是它可以很容易的看到哪个字段应用了哪个分析器。
*.通常情况下，大部分的字符串字段是确切值not_analyzed字段（索引但不分析字段）比如标签，枚举，加上少数全文检索字段会使用默认的分析器，像standard或者 english或者其他语言。然后你可能只有一到两个字段需要定制分析。
*.你可以在索引设置default分析器的地方为几乎所有全文检索字段设置成你想要的分析器，并且只要在一到两个字段指定专门的分析器。如果在你的模型中，你的每个类型都需要不同的分析器，那么在类型级别使用analyzer配置来代替。
*.一个普通的像日志一样的基于时间轴的工作流数据每天都得创建新的索引，这种场景你可以使用索引模板来指定新的索引的配置和映射。
*.一个问题的描述与解释：：问题：创建了一些文档，执行了一个简单的查询，结果发现相关性较低的结果排在了相关性较高的结果的前面。解释：Elasticsearch默认的相关性算法，叫做词频率/反转文档频率（TF/IDF）。词频率是一个词在我们当前查询的文档的字段中出现的次数。出现的次数越多，相关性就越大。反转文档频率指的是该索引中所有文档数与出现这个词的文档数的百分比，词出现的频率越大，IDF越小。由于性能原因，Elasticsearch不通过索引中所有的文档计算IDF，而是每个分片会为分片中所有的文档计算一个本地的IDF取而代之。在一个索引中索引的文档越多，本地IDF和全局IDF的区别就会越少，所以文档很多的情况下一般没有问题，但文档很少时（比如测试时）就会表现出问题。解决：创建一个只有一个主分片的索引，这样，本地IDF就是全局IDF；在请求中添加?search_type=dfs_query_then_fetch。dfs就是Distributed Frequency Search，它会告诉Elasticsearch检查每一个分片的本地IDF以计算整个索引的全局IDF。提示：不要把dfs_query_then_fetch用于生产环境。它实在是没有必要。只要有足够的数据就能够确保词频率很好的分布。
*.只有一个简单的match子句的查询是很少的。我们经常需要在一个或者多个字段中查询相同的或者不同的查询字符串，这意味着我们需要能够组合多个查询子句以及使他们的相关性得分有意义。
*.用于构建多个查询子句搜索的可能的工具，以及怎么样选择解决方案来应用到你特殊的场景。
*.在明确的字段中的词查询是最容易处理的多字段查询。如果我们知道War and Peace是标题，Leo Tolstoy是作者，则可以很容易的用match查询表达每个条件，并且用布尔查询组合起来
*.布尔查询采用"匹配越多越好(More-matches-is-better)"的方法，所以每个match子句的得分会被加起来变成最后的每个文档的得分。匹配两个子句的文档的得分会比只匹配了一个文档的得分高。
*.布尔查询可以包装任何其他的查询类型，包含其他的布尔查询
*.我们把翻译者的子句放在一个独立的布尔查询中，而不是把翻译者的子句放在和title以及作者的同一级，是因为计算得分的需要：：布尔查询执行每个匹配查询，把他们的得分加在一起，然后乘以匹配子句的数量，并且除以子句的总数。每个同级的子句权重是相同的。
*.我们可能对标题以及作者比翻译者更感兴趣。我们需要调整查询来使得标题和作者的子句相关性更重要。最简单的方法是使用boost参数。
*.通过试错(Trial and Error)的方式可以确定"最佳"的boost值，一个合理boost值的范围在1和10之间，也可能是15。比它更高的值的影响不会起到很大的作用，因为分值会被规范化(Normalized)。
*.bool查询是多字段查询的中流砥柱。在很多场合下它都能很好地工作，特别是当你能够将不同的查询字符串映射到不同的字段时。
*.现在的用户期望能够在一个地方输入所有的搜索词条，然后应用能够知道如何为他们得到正确的结果。
*.对于多词，多字段查询并没有一种万能(one-size-fits-all)的方法。要得到最佳的结果，你需要了解你的数据以及如何使用恰当的工具。
*.一个用来调优相关度的常用技术是将相同的数据索引到多个字段中，每个字段拥有自己的分析链(Analysis Chain)。主要字段会含有单词的词干部分，同义词和消除了变音符号的单词。它用来尽可能多地匹配文档。相同的文本可以被索引到其它的字段中来提供更加精确的匹配。一个字段或许会包含未被提取成词干的单词，另一个字段是包含了变音符号的单词，第三个字段则使用shingle来提供关于单词邻近度(Word Proximity)的信息。以上这些额外的字段扮演着signal的角色，用来增加每个匹配的文档的相关度分值。越多的字段被匹配则意味着文档的相关度越高。
*.对于一些实体，标识信息会在多个字段中出现，每个字段中只含有一部分信息，此时，我们希望在任意字段中找到尽可能多的单词。我们需要在多个字段中进行查询，就好像这些字段是一个字段那样。
*.用户输入了"Brown fox"，然后按下了搜索键。我们无法预先知道用户搜索的词条会出现在博文的title或者body字段中，但是用户是在搜索和他输入的单词相关的内容。
*.bool查询是如何计算得到其分值的：
	1.运行should子句中的两个查询
	2.相加查询返回的分值
	3.将相加得到的分值乘以匹配的查询子句的数量
	4.除以总的查询子句的数量
*.有时我们不想合并来自每个字段的分值，而是使用最佳匹配字段的分值作为整个查询的整体分值。这就会让包含有我们寻找的两个单词的字段有更高的权重，而不是在不同的字段中重复出现的相同单词。
*.相比使用bool查询，我们可以使用dis_max查询(Disjuction Max Query)。返回匹配了任何查询的文档，并且分值是产生了最佳匹配的查询所对应的分值。
*.bool查询的相关性得分计算：加-乘-除；dis_max查询的相关性得分计算：将最高的子句得分作为整个查询的整体得分；
*.简单的dis_max查询只是简单的使用最佳匹配查询子句得到的_score。通过指定tie_breaker参数，也可以将其它匹配的查询子句考虑进来。
*.tie_breaker参数会让dis_max查询的行为更像是dis_max和bool的一种折中。它会通过下面的方式改变分值计算过程：
	1.取得最佳匹配查询子句的_score。
	2.将其它每个匹配的子句的分值乘以tie_breaker。
	3.将以上得到的分值进行累加并规范化。
*.通过tie_breaker参数，所有匹配的子句都会起作用，只不过最佳匹配子句的作用更大。提示：tie_breaker的取值范围是0到1之间的浮点数，取0时即为仅使用最佳匹配子句(译注：这和不使用tie_breaker参数的dis_max查询效果相同)，取1则会将所有匹配的子句一视同仁。它的确切值需要根据你的数据和查询进行调整，但是一个合理的值会靠近0，(比如，0.1 -0.4)，来确保不会压倒dis_max查询具有的最佳匹配性质。
*.multi_match查询提供了一个简便的方法用来对多个字段执行相同的查询。默认情况下，multi_match查询以best_fields类型执行，它会为每个字段生成一个match查询，然后将这些查询包含在一个dis_max查询中。
*.存在几种类型的multi_match查询：：best_fields，most_fields以及cross_fields。
*.使用multi_match进行查询时，字段名可以通过通配符指定：任何匹配了通配符的字段都会被包含在搜索中。
*.个别字段可以通过caret语法(^)进行加权：仅需要在字段名后添加^boost，其中的boost是一个浮点数。
*.全文搜索是一场召回率(Recall)--返回所有相关的文档，与准确率(Precision)--不返回无关文档，之间的战斗。目标是在结果的第一页给用户呈现最相关的文档。
*.为了提高召回率，我们会广撒网--不仅包括精确匹配了用户搜索词条的文档，还包括了那些我们认为和查询相关的文档。如果一个用户搜索了"quick brown fox"，一份含有fast foxes的文档也可以作为一个合理的返回结果。
*.在包含了许多可能的匹配后，我们需要确保相关度高的文档出现在顶部。
*.一个用来调优全文搜索相关性的常用技术是将同样的文本以多种方式索引，每一种索引方式都提供了不同相关度的信号(Signal)。主要字段(Main field)中含有的词条的形式是最宽泛的(Broadest-matching)，用来尽可能多的匹配文档。比如如下一些方法：：
	1.使用一个词干提取器来将jumps，jumping和jumped索引成它们的词根：jump。然后当用户搜索的是jumped时，我们仍然能够匹配含有jumping的文档。
	2.包含同义词，比如jump，leap和hop。
	3.移除变音符号或者声调符号
*.如果我们有两份文档，其中之一含有jumped，而另一份含有jumping，那么用户会希望第一份文档的排序会靠前，因为它含有用户输入的精确值jumped。
*.我们可以通过将相同的文本索引到其它字段来提供更加精确的匹配。一个字段可以包含未被提取词干的版本，另一个则是含有变音符号的原始单词，然后第三个使用了shingles，用来提供和单词邻近度相关的信息。这些其它字段扮演的角色就是信号(Signals)，它们用来增加每个匹配文档的相关度分值。能够匹配的字段越多，相关度就越高。
*.如果一份文档能够匹配具有最宽泛形式的主要字段(Main field)，那么它就会被包含到结果列表中。如果它同时也匹配了信号字段，它会得到一些额外的分值用来将它移动到结果列表的前面。
*.我们会在本书的后面讨论同义词，单词邻近度，部分匹配以及其他可能的信号，但是我们会使用提取了词干和未提取词干的字段的简单例子来解释这个技术。
*.第一件事就是将我们的字段索引两次：一次是提取了词干的形式，一次是未提取词干的形式。为了实现它，我们会使用多字段(Multifields)。
*.<2> title字段使用了english解析器进行词干提取。<3> title.std字段则使用的是standard解析器，因此它没有进行词干提取。
*.111.简单的针对title字段的match查询，它查询jumping rabbits，那么它会变成一个针对两个提干后的词条jump和rabbit的查询，这要得益于english解析器。两份文档的title字段都包含了以上两个词条，因此两份文档的分值是相同的。222.如果我们只查询title.std字段，那么只有文档2会匹配。333.当我们查询两个字段并将它们的分值通过bool查询进行合并的话，两份文档都能够匹配(title字段也匹配了)，而文档2的分值会更高一些(匹配了title.std字段)。
*.在上述查询中，由于我们想合并所有匹配字段的分值，因此使用的类型为most_fields。这会让multi_match查询将针对两个字段的查询子句包含在一个bool查询中，而不是包含在一个dis_max查询中。
*.我们使用了拥有宽泛形式的title字段来匹配尽可能多的文档--来增加召回率(Recall)，同时也使用了title.std字段作为信号来让最相关的文档能够拥有更靠前的排序(译注：--来增加准确率(Precision))。
*.每个字段对最终分值的贡献可以通过指定boost值进行控制。比如，我们可以提升title字段来让该字段更加重要，这也减小了其它信号字段的影响
*.跨字段实体搜索，类似person，product或者address这样的实体，它们的信息会分散到多个字段中。
*.实际上，我们可以使用最笨的方法，依次查询每个字段然后将每个匹配字段的分值进行累加，这听起来很像bool查询能够胜任的工作。
*.对每个字段重复查询字符串很快就会显得冗长，我们可以使用multi_match查询进行替代，然后将type设置为most_fields来让它将所有匹配字段的分值合并。
*.上述提到的三个问题都来源于most_fields是以字段为中心(Field-centric)，而不是以词条为中心(Term-centric)：它会查询最多匹配的字段(Most matching fields)，而我们真正感兴趣的最匹配的词条(Most matching terms)。
*.考虑一下most_fields查询是如何执行的：ES会为每个字段生成一个match查询，然后将它们包含在一个bool查询中。
*.你可以发现能够在两个字段中匹配poland的文档会比在一个字段中匹配了poland和street的文档的分值要高。
*.这个问题仅在我们处理多字段时存在。如果我们将所有这些字段合并到一个字段中，该问题就不复存在了。我们可以向person文档中添加一个full_name字段来实现。
*.特殊的_all字段会将其它所有字段中的值作为一个大字符串进行索引。将所有字段的值作为一个字段进行索引并不是非常灵活。ES通过字段映射中的copy_to参数向我们提供了自定义_all字段功能。
*.有了这个copy_to映射，我们可以通过first_name字段查询名字，last_name字段查询姓氏，或者full_name字段查询姓氏和名字。
*.提示：first_name和last_name字段的映射和full_name字段的索引方式的无关。full_name字段会从其它两个字段中拷贝字符串的值，然后仅根据full_name字段自身的映射进行索引。
*.ES同时也提供了一个搜索期间的解决方案：使用类型为cross_fields的multi_match查询。cross_fields类型的multi_match查询采用了一种以词条为中心(Term-centric)的方法，这种方法和best_fields及most_fields采用的以字段为中心(Field-centric)的方法有很大的区别。它将所有的字段视为一个大的字段，然后在任一字段中搜索每个词条。
*.以字段为中心的查询和以词条为中心的查询。
*.cross_fields类型的multi_match查询首先会解析查询字符串来得到一个词条列表，然后在任一字段中搜索每个词条。
*.换言之，它会查找词条smith在first_name和last_name字段中的IDF值，然后使用两者中较小的作为两个字段最终的IDF值。因为smith是一个常见的姓氏，意味着它也会被当做一个常见的名字。
*.提示：为了让cross_fields查询类型能以最佳的方式工作，所有的字段都需要使用相同的解析器。使用了相同的解析器的字段会被组合在一起形成混合字段(Blended Fields)。
*.使用cross_fields查询相比使用自定义_all字段的一个优点是你能够在查询期间对个别字段进行加权。对于first_name和last_name这类拥有近似值的字段，也许加权是不必要的，如果你通过title和description字段来搜索书籍，那么你或许会给予title字段更多的权重。这可以通过前面介绍的caret(^)语法来完成。
*.在multi_match查询中将not_analyzed字段混合到analyzed字段中是没有益处的。在multi_match查询中避免使用not_analyzed字段。



以下为Java客户端使用方法：：
1.创建client实例：
Settings settings = Settings.builder()
        .put("cluster.name", "myClusterName").build();
TransportClient client = new PreBuiltTransportClient(settings);
// on startup
TransportClient client = new PreBuiltTransportClient(settings)
        .addTransportAddress(new InetSocketTransportAddress(InetAddress.getByName("host1"), 9300))
        .addTransportAddress(new InetSocketTransportAddress(InetAddress.getByName("host2"), 9300));
// on shutdown
client.close();

2.使用client实例进行基本的操作：
IndexResponse response = client.prepareIndex("twitter", "tweet", "1").setSource().get();
GetResponse response = client.prepareGet("twitter", "tweet", "1").get();
DeleteResponse response = client.prepareDelete("twitter", "tweet", "1").get();
SearchResponse response = client.prepareSearch("index1", "index2")
        .setTypes("type1", "type2")
        .setSearchType(SearchType.DFS_QUERY_THEN_FETCH)
        .setQuery(QueryBuilders.termQuery("multi", "test"))                 // Query
        .setPostFilter(QueryBuilders.rangeQuery("age").from(12).to(18))     // Filter
        .setFrom(0).setSize(60).setExplain(true)
        .get();
